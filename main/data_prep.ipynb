{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
      "Requirement already satisfied: numpy in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: sklearn in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 3)) (0.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../paysim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "# Define the distribution percentages\n",
    "percentage_isFraud_0 = 99.87\n",
    "percentage_isFraud_1 = 0.13\n",
    "\n",
    "# Calculate the number of instances for each category based on the desired percentages\n",
    "total_sample_size = 500  # You can adjust this as per your requirement\n",
    "num_isFraud_0 = int(total_sample_size * percentage_isFraud_0 / 100)\n",
    "num_isFraud_1 = total_sample_size - num_isFraud_0\n",
    "\n",
    "# Generate the sample DataFrame with the desired distribution\n",
    "sample_isFraud_0 = df[df['isFraud'] == 0].sample(n=num_isFraud_0, replace=True)\n",
    "sample_isFraud_1 = df[df['isFraud'] == 1].sample(n=num_isFraud_1, replace=True)\n",
    "\n",
    "# Concatenate the samples to form the final sample DataFrame\n",
    "sample = pd.concat([sample_isFraud_0, sample_isFraud_1])\n",
    "\n",
    "# Shuffle the rows to randomize the order\n",
    "df = sample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'oldbalanceOrg': 'oldbalanceOrig'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new balance and original balance based on transaction amount based on EDA\n",
    "# Percentage of observations with balance errors in the account giving money:  85.0\n",
    "# Percentage of observations with balance errors in the account receiving money:  100.0\n",
    "\n",
    "df['newbalanceDest'] = df['oldbalanceDest'] + df['amount']\n",
    "df['oldbalanceOrig'] = df['newbalanceOrig'] + df['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only 6 true\n",
    "# df['externalDest'] = ((df['oldbalanceDest'] == 0) & (df['newbalanceDest'] == 0)).astype(int)\n",
    "# # Only 16 true\n",
    "# df['externalOrig'] = ((df['oldbalanceOrig'] == 0) & (df['newbalanceOrig'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hour of the day from the 'step' column\n",
    "df['hour'] = df['step']% 24\n",
    "\n",
    "# Extracting day of the week as integers, add 3 to convert it to correct days of the week (1 = monday, 7 = sunday)\n",
    "df['weekday'] = (df['step'] // 24) % 7 + 1\n",
    "\n",
    "# Create is_workday feature based on the 2 least transaction dates being the weekend\n",
    "df['is_workday'] = df['weekday'].apply(lambda x: 0 if x == 4 or x == 5 else 1)\n",
    "\n",
    "# Extracting day of the week as integers\n",
    "df['monthday'] = (df['step'] % 30) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rolling average of last 3 and 7 transactions for each recipient\n",
    "df['meanDest3'] = df.groupby('nameDest')['amount'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "df['meanDest7'] = df.groupby('nameDest')['amount'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# calculate the rolling maximum of last 3 and 7 transactions for each recipient\n",
    "df['maxDest3'] = df.groupby('nameDest')['amount'].rolling(window=3, min_periods=1).max().reset_index(0, drop=True)\n",
    "df['maxDest7'] = df.groupby('nameDest')['amount'].rolling(window=7, min_periods=1).max().reset_index(0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new type column indicatin if transaction was from Customer (C) to Merchant (M) or any other combination\n",
    "\n",
    "conditions = [\n",
    "    (df['nameOrig'].str.contains('C')) & (df['nameDest'].str.contains('C')),\n",
    "    (df['nameOrig'].str.contains('C')) & (df['nameDest'].str.contains('M')),\n",
    "    (df['nameOrig'].str.contains('M')) & (df['nameDest'].str.contains('C')),\n",
    "    (df['nameOrig'].str.contains('M')) & (df['nameDest'].str.contains('M'))\n",
    "]\n",
    "\n",
    "choices = ['CC', 'CM', 'MC', 'MM']\n",
    "\n",
    "df['type2'] = np.select(conditions, choices, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One hot encode type columns\n",
    "df = pd.get_dummies(df, columns=['type', 'type2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale amount\n",
    "df['log_amount'] = np.log(df['amount'])\n",
    "df = df[['log_amount'] + [col for col in df.columns if col != 'log_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fuzzy_sets_percentile(column_data, percentiles):\n",
    "    fuzzy_sets = []\n",
    "    for value in column_data:\n",
    "        if value <= percentiles[0.2]:\n",
    "            fuzzy_sets.append(0.0)\n",
    "        elif value <= percentiles[0.4]:\n",
    "            fuzzy_sets.append(0.3)\n",
    "        elif value <= percentiles[0.6]:\n",
    "            fuzzy_sets.append(0.5)\n",
    "        elif value <= percentiles[0.8]:\n",
    "            fuzzy_sets.append(0.7)\n",
    "        else:\n",
    "            fuzzy_sets.append(1.0)\n",
    "    return fuzzy_sets\n",
    "\n",
    "# Calculate percentiles for each column\n",
    "percentiles = df.quantile([0.2, 0.4, 0.6, 0.8]).to_dict()\n",
    "\n",
    "# Create fuzzy sets for each column\n",
    "fuzzy_sets_percentile = {}\n",
    "for column in df.columns:\n",
    "    fuzzy_sets_percentile[column] = create_fuzzy_sets_percentile(df[column], percentiles[column])\n",
    "\n",
    "# Overwrite the values in the DataFrame with the new fuzzy sets values\n",
    "for column in df.columns:\n",
    "    df[column] = fuzzy_sets_percentile[column]\n",
    "\n",
    "# Print or use the DataFrame with fuzzy sets values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud', 'amount', 'hour', 'weekday', 'monthday'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the 'isFraud' column to the end of the dataframe to become Y column\n",
    "is_fraud_col = df.pop('isFraud')\n",
    "df['isFraud'] = is_fraud_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test set following 0.7/0.15/0.15 split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=0, shuffle=True)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_directory = \"../deep-symbolic-optimization/dso/dso/task/regression/data\"\n",
    "\n",
    "# # Check if the directory exists, if not, create it\n",
    "# if not os.path.exists(save_directory):\n",
    "#     os.makedirs(save_directory)\n",
    "\n",
    "# # Save DataFrames to CSV files\n",
    "# train_df.to_csv(os.path.join(save_directory, \"train_df.csv\"), header = False, index=False)\n",
    "# test_df.to_csv(os.path.join(save_directory, \"test_df.csv\"), header = False, index=False)\n",
    "# val_df.to_csv(os.path.join(save_directory, \"val_df.csv\"), header = False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
