{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
      "Requirement already satisfied: numpy in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: sklearn in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: scikit-learn in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 3)) (0.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/wout/Documents/UvA/Thesis/.venv/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../paysim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "# Define the distribution percentages\n",
    "percentage_isFraud_0 = 99.87\n",
    "percentage_isFraud_1 = 0.13\n",
    "\n",
    "# Calculate the number of instances for each category based on the desired percentages\n",
    "total_sample_size = 20000  # You can adjust this as per your requirement\n",
    "num_isFraud_0 = int(total_sample_size * percentage_isFraud_0 / 100)\n",
    "num_isFraud_1 = total_sample_size - num_isFraud_0\n",
    "\n",
    "# Generate the sample DataFrame with the desired distribution\n",
    "sample_isFraud_0 = df[df['isFraud'] == 0].sample(n=num_isFraud_0, replace=True)\n",
    "sample_isFraud_1 = df[df['isFraud'] == 1].sample(n=num_isFraud_1, replace=True)\n",
    "\n",
    "# Concatenate the samples to form the final sample DataFrame\n",
    "sample = pd.concat([sample_isFraud_0, sample_isFraud_1])\n",
    "\n",
    "# Shuffle the rows to randomize the order\n",
    "df = sample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'oldbalanceOrg': 'oldbalanceOrig'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new balance and original balance based on transaction amount based on EDA\n",
    "# Percentage of observations with balance errors in the account giving money:  85.0\n",
    "# Percentage of observations with balance errors in the account receiving money:  100.0\n",
    "\n",
    "df['newbalanceDest'] = df['oldbalanceDest'] + df['amount']\n",
    "df['oldbalanceOrig'] = df['newbalanceOrig'] + df['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only 6 true\n",
    "# df['externalDest'] = ((df['oldbalanceDest'] == 0) & (df['newbalanceDest'] == 0)).astype(int)\n",
    "# # Only 16 true\n",
    "# df['externalOrig'] = ((df['oldbalanceOrig'] == 0) & (df['newbalanceOrig'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hour of the day from the 'step' column\n",
    "df['hour'] = df['step']% 24\n",
    "\n",
    "# Extracting day of the week as integers, add 3 to convert it to correct days of the week (1 = monday, 7 = sunday)\n",
    "df['weekday'] = (df['step'] // 24) % 7 + 1\n",
    "\n",
    "# Create is_workday feature based on the 2 least transaction dates being the weekend\n",
    "df['is_workday'] = df['weekday'].apply(lambda x: 0 if x == 4 or x == 5 else 1)\n",
    "\n",
    "# Extracting day of the week as integers\n",
    "df['monthday'] = (df['step'] % 30) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rolling average of last 3 and 7 transactions for each recipient\n",
    "df['meanDest3'] = df.groupby('nameDest')['amount'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "df['meanDest7'] = df.groupby('nameDest')['amount'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# calculate the rolling maximum of last 3 and 7 transactions for each recipient\n",
    "df['maxDest3'] = df.groupby('nameDest')['amount'].rolling(window=3, min_periods=1).max().reset_index(0, drop=True)\n",
    "df['maxDest7'] = df.groupby('nameDest')['amount'].rolling(window=7, min_periods=1).max().reset_index(0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new type column indicatin if transaction was from Customer (C) to Merchant (M) or any other combination\n",
    "\n",
    "conditions = [\n",
    "    (df['nameOrig'].str.contains('C')) & (df['nameDest'].str.contains('C')),\n",
    "    (df['nameOrig'].str.contains('C')) & (df['nameDest'].str.contains('M')),\n",
    "    (df['nameOrig'].str.contains('M')) & (df['nameDest'].str.contains('C')),\n",
    "    (df['nameOrig'].str.contains('M')) & (df['nameDest'].str.contains('M'))\n",
    "]\n",
    "\n",
    "choices = ['CC', 'CM', 'MC', 'MM']\n",
    "\n",
    "df['type2'] = np.select(conditions, choices, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One hot encode type columns\n",
    "df = pd.get_dummies(df, columns=['type', 'type2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale amount\n",
    "df['log_amount'] = np.log(df['amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the 'isFraud' column to the end of the dataframe to become Y column\n",
    "is_fraud_col = df.pop('isFraud')\n",
    "df['isFraud'] = is_fraud_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test set following 0.7/0.15/0.15 split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=0, shuffle=True)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../deep-symbolic-optimization/dso/dso/task/regression/data\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "train_df.to_csv(os.path.join(save_directory, \"train_df.csv\"), header = False, index=False)\n",
    "test_df.to_csv(os.path.join(save_directory, \"test_df.csv\"), header = False, index=False)\n",
    "val_df.to_csv(os.path.join(save_directory, \"val_df.csv\"), header = False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
